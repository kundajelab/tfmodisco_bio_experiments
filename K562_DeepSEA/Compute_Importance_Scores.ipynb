{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 16:38:50.595997 140706238011136 deprecation_wrapper.py:119] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0823 16:38:50.628729 140706238011136 deprecation_wrapper.py:119] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0823 16:38:50.681700 140706238011136 deprecation_wrapper.py:119] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0823 16:38:50.682928 140706238011136 deprecation_wrapper.py:119] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0823 16:38:50.692144 140706238011136 deprecation.py:506] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0823 16:38:50.711700 140706238011136 deprecation_wrapper.py:119] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0823 16:38:51.316439 140706238011136 deprecation_wrapper.py:119] From /users/avanti/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "/users/avanti/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "deepsea_model = load_model(\"/srv/scratch/avanti/ExPecto/resources/deepseabeluga_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1993, 320)         10560     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1993, 320)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1986, 320)         819520    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1986, 320)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1986, 320)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 496, 320)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 489, 480)          1229280   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 489, 480)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 482, 480)          1843680   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 482, 480)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 482, 480)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 120, 480)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 113, 640)          2458240   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 113, 640)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 106, 640)          3277440   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 106, 640)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 106, 640)          0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 640, 106)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67840)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2003)              135885523 \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2003)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2002)              4012008   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2002)              0         \n",
      "=================================================================\n",
      "Total params: 149,536,251\n",
      "Trainable params: 149,536,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deepsea_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
      "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import seqdataloader\n",
    "from seqdataloader.batchproducers.coordbased.coordbatchproducers import SimpleCoordsBatchProducer\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.fasta import PyfaidxCoordsToVals\n",
    "\n",
    "positives_coords_batch_producer = SimpleCoordsBatchProducer(\n",
    "    bed_file=\"2000bp_around_hg38_new_k562.narrowPeak.gz\",\n",
    "    batch_size=64)\n",
    "negatives_coords_batch_producer = SimpleCoordsBatchProducer(\n",
    "    bed_file=\"nok562_2000bp_around_hg38_merged_universal_neg_representative_peaks.bed.gz\",\n",
    "    batch_size=64)\n",
    "coords_to_onehot = PyfaidxCoordsToVals(genome_fasta_path=\"hg38.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0\n",
      "Done 6400\n",
      "Done 12800\n",
      "Done 19200\n",
      "Done 25600\n",
      "Done 32000\n",
      "Done 38400\n",
      "Done 44800\n",
      "Done 51200\n",
      "Done 57600\n",
      "Done 64000\n",
      "Done 70400\n",
      "Done 76800\n",
      "Done 83200\n",
      "Done 89600\n",
      "Done 96000\n",
      "Done 102400\n",
      "Done 108800\n",
      "Done 115200\n",
      "Done 121600\n",
      "Done 128000\n",
      "Done 134400\n",
      "Done 140800\n",
      "Done 147200\n",
      "Done 153600\n",
      "Done 160000\n",
      "Done 166400\n",
      "Done 172800\n",
      "Done 179200\n",
      "Done 185600\n",
      "Done 192000\n",
      "Done 198400\n",
      "Done 204800\n"
     ]
    }
   ],
   "source": [
    "positives_preds = []\n",
    "for batch_idx in range(len(positives_coords_batch_producer)):\n",
    "    if (batch_idx%100==0):\n",
    "        print(\"Done\",len(positives_preds))\n",
    "    positives_preds.extend(deepsea_model.predict(\n",
    "                            coords_to_onehot(positives_coords_batch_producer[batch_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0\n",
      "Done 6400\n",
      "Done 12800\n",
      "Done 19200\n",
      "Done 25600\n",
      "Done 32000\n",
      "Done 38400\n",
      "Done 44800\n",
      "Done 51200\n",
      "Done 57600\n",
      "Done 64000\n",
      "Done 70400\n",
      "Done 76800\n"
     ]
    }
   ],
   "source": [
    "negatives_preds = []\n",
    "for batch_idx in range(len(negatives_coords_batch_producer)):\n",
    "    if (batch_idx%100==0):\n",
    "        print(\"Done\",len(negatives_preds))\n",
    "    negatives_preds.extend(deepsea_model.predict(\n",
    "                            coords_to_onehot(negatives_coords_batch_producer[batch_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "y_score = list(np.array(positives_preds)[:,61])+list(np.array(negatives_preds)[:,61])\n",
    "y_true = np.array([1 for x in positives_preds]\n",
    "                  +[0 for x in negatives_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "shuffle_several_times = lambda s: np.array([dinuc_shuffle(s) for i in range(20)])\n",
    "\n",
    "#This combine_mult_and_diffref function can be used to generate hypothetical\n",
    "# importance scores for one-hot encoded sequence.\n",
    "#Hypothetical scores can be thought of as quick estimates of what the\n",
    "# contribution *would have been* if a different base were present. Hypothetical\n",
    "# scores are used as input to the importance score clustering algorithm\n",
    "# TF-MoDISco (https://github.com/kundajelab/tfmodisco)\n",
    "# Hypothetical importance scores are discussed more in this pull request:\n",
    "#  https://github.com/kundajelab/deeplift/pull/36\n",
    "def combine_mult_and_diffref(mult, orig_inp, bg_data):\n",
    "    to_return = []\n",
    "    for l in range(len(mult)):\n",
    "        projected_hypothetical_contribs = np.zeros_like(bg_data[l]).astype(\"float\")\n",
    "        assert len(orig_inp[l].shape)==2\n",
    "        #At each position in the input sequence, we iterate over the one-hot encoding\n",
    "        # possibilities (eg: for genomic sequence, this is ACGT i.e.\n",
    "        # 1000, 0100, 0010 and 0001) and compute the hypothetical \n",
    "        # difference-from-reference in each case. We then multiply the hypothetical\n",
    "        # differences-from-reference with the multipliers to get the hypothetical contributions.\n",
    "        #For each of the one-hot encoding possibilities,\n",
    "        # the hypothetical contributions are then summed across the ACGT axis to estimate\n",
    "        # the total hypothetical contribution of each position. This per-position hypothetical\n",
    "        # contribution is then assigned (\"projected\") onto whichever base was present in the\n",
    "        # hypothetical sequence.\n",
    "        #The reason this is a fast estimate of what the importance scores *would* look\n",
    "        # like if different bases were present in the underlying sequence is that\n",
    "        # the multipliers are computed once using the original sequence, and are not\n",
    "        # computed again for each hypothetical sequence.\n",
    "        for i in range(orig_inp[l].shape[-1]):\n",
    "            hypothetical_input = np.zeros_like(orig_inp[l]).astype(\"float\")\n",
    "            hypothetical_input[:,i] = 1.0\n",
    "            hypothetical_difference_from_reference = (hypothetical_input[None,:,:]-bg_data[l])\n",
    "            hypothetical_contribs = hypothetical_difference_from_reference*mult[l]\n",
    "            projected_hypothetical_contribs[:,:,i] = np.sum(hypothetical_contribs,axis=-1) \n",
    "        to_return.append(np.mean(projected_hypothetical_contribs,axis=0))\n",
    "    return to_return\n",
    "\n",
    "dinuc_shuff_explainer = shap.DeepExplainer(\n",
    "    #Importance is computed w.r.t. layers[-2] because that corresponds to the logit\n",
    "    # of the sigmoid; we do this to avoid plateauing of importance scores due to\n",
    "    # saturation of the sigmoid, as explained in the DeepLIFT paper\n",
    "    #Task 61 because this is K562 as per\n",
    "    # https://github.com/FunctionLab/ExPecto/blob/master/resources/deepsea_beluga_2002_features.tsv\n",
    "    (deepsea_model.input, deepsea_model.layers[-2].output[:,61]),\n",
    "    shuffle_several_times,\n",
    "    combine_mult_and_diffref=combine_mult_and_diffref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(\"scores.h5\", mode=\"r+\")\n",
    "f.create_dataset(\n",
    "    \"hyp_imp\",\n",
    "    (len(positives_coords_batch_producer.coords_list), 2000,4),\n",
    "    dtype=\"float32\")\n",
    "f.create_dataset(\n",
    "    \"seq\",\n",
    "    (len(positives_coords_batch_producer.coords_list), 2000,4),\n",
    "    dtype=\"float32\")\n",
    "\n",
    "num_done = 0\n",
    "for batch_idx in range(len(positives_coords_batch_producer)):\n",
    "    if (batch_idx%10==0):\n",
    "        print(\"Done\",len(positives_deepshap))\n",
    "    onehot = coords_to_onehot(positives_coords_batch_producer[batch_idx])\n",
    "    f[\"seq\"][num_done:num_done+len(onehot)] = onehot\n",
    "    hyp_imp = dinuc_shuff_explainer.shap_values(onehot)\n",
    "    f[\"hyp_imp\"][num_done:num_done+len(hypimp)] = hyp_imp\n",
    "    num_done += len(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
