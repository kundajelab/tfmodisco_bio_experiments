Evalutation of best performing model:
[0.43884479498436535, 0.5833581069029075]
Best performing model chosen hyper-parameters index:
{'kernel_size1': 3, 'n_conv_1': 3, 'Dense': 1, 'Dropout': 1, 'n_conv': 0, 'lr': 2, 'filters': 0, 'filters_1': 0, 'filters_2': 2}

{'kernel_size1':3=25, 'n_conv_1 (n_dense)':3=3, 'Dense':1=100, 'Dropout': 1=0.4, 'n_conv':0=0, 'lr':2=0.0001, 'filters': 0=50, 'filters_1': 0=50, 'filters_2(pool_size)': 2=60}

Summary of best model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_69 (Conv1D)           (None, 976, 50)           5050      
_________________________________________________________________
batch_normalization_124 (Bat (None, 976, 50)           200       
_________________________________________________________________
activation_159 (Activation)  (None, 976, 50)           0         
_________________________________________________________________
max_pooling1d_36 (MaxPooling (None, 16, 50)            0         
_________________________________________________________________
flatten_36 (Flatten)         (None, 800)               0         
_________________________________________________________________
dense_91 (Dense)             (None, 100)               80100     
_________________________________________________________________
batch_normalization_125 (Bat (None, 100)               400       
_________________________________________________________________
activation_160 (Activation)  (None, 100)               0         
_________________________________________________________________
dropout_56 (Dropout)         (None, 100)               0         
_________________________________________________________________
dense_92 (Dense)             (None, 100)               10100     
_________________________________________________________________
batch_normalization_126 (Bat (None, 100)               400       
_________________________________________________________________
activation_161 (Activation)  (None, 100)               0         
_________________________________________________________________
dropout_57 (Dropout)         (None, 100)               0         
_________________________________________________________________
dense_93 (Dense)             (None, 100)               10100     
_________________________________________________________________
batch_normalization_127 (Bat (None, 100)               400       
_________________________________________________________________
activation_162 (Activation)  (None, 100)               0         
_________________________________________________________________
dropout_58 (Dropout)         (None, 100)               0         
_________________________________________________________________
dense_94 (Dense)             (None, 4)                 404       
_________________________________________________________________
activation_163 (Activation)  (None, 4)                 0         
=================================================================
Total params: 107,154
Trainable params: 106,454
Non-trainable params: 700
_________________________________________________________________
>>> Hyperas search space:

def get_space():
    return {
        'kernel_size1': hp.choice('kernel_size1', [13, 15, 19, 25, 39]),
        'filters': hp.choice('filters', [50,100,250,500,1000]),
        'n_conv': hp.choice('n_conv', [0,1,2,3]),
        'filters_1': hp.choice('filters_1', [50,100,250,500,1000]),
        'filters_2': hp.choice('filters_2', [10,30,60]),
        'n_conv_1': hp.choice('n_conv_1', [0,1,2,3]),
        'Dense': hp.choice('Dense', [50,100,200]),
        'Dropout': hp.choice('Dropout', [0.2,0.4,0.6]),
        'lr': hp.choice('lr', [0.01, 0.001, 0.0001]),
    }

>>> Data
  1: 
  2: np.random.seed(1234)  # for reproducibility
  3: data=np.load('../data_big.npz')
  4: x_train=data['arr_0']
  5: y_train=data['arr_1']
  6: x_validate=data['arr_2']
  7: y_validate=data['arr_3']
  8: x_test=data['arr_4']
  9: y_test=data['arr_5']
 10: 
 11: 
 12: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     start = int(time.time())
   4:     np.random.seed(1234)
   5:     try:
   6:         '''
   7:         b) with and without a hidden fully-connected layer, 
   8:         c) number of units in the hidden fc layer < 200, 
   9:         c) different learning rates for adam (explore on a log scale - 0.001, 0.0001, etc), 
  10:         d) maxpooling widths in the 10-60 range, 
  11:         e) conv widths in the 10-40 range.
  12:         '''
  13:         model=Sequential()
  14:         kernel_size1 = space['kernel_size1']
  15:         kernel_size2 = kernel_size1 - 2
  16:         model.add(Conv1D(filters=space['filters'],kernel_size=(kernel_size1),input_shape=(1000,4)))
  17:         model.add(BatchNormalization(axis=-1))
  18:         model.add(Activation('relu'))
  19: 
  20:         ## a) number of layers between 1 and 4, 
  21: 
  22:         #decide on how many conv layers in model 
  23:         n_conv = space['n_conv']
  24: 
  25:         filter_dim=[kernel_size1,kernel_size2,kernel_size2]
  26: 
  27:         for i in range(n_conv):
  28:             model.add(Conv1D(filters=space['filters_1'],kernel_size=(filter_dim[i])))
  29:             model.add(BatchNormalization(axis=-1))
  30:             model.add(Activation('relu'))
  31: 
  32:         model.add(MaxPooling1D(pool_size=(space['filters_2'])))
  33: 
  34:         model.add(Flatten())
  35:         n_dense = space['n_conv_1']
  36:         for i in range(n_dense):
  37:             model.add(Dense(space['Dense']))
  38:             model.add(BatchNormalization(axis=-1))
  39:             model.add(Activation('relu'))
  40:             model.add(Dropout(space['Dropout']))
  41: 
  42:         model.add(Dense(4))
  43:         model.add(Activation("sigmoid"))
  44: 
  45:         adam=keras.optimizers.Adam(lr=space['lr'])
  46: 
  47:         model.compile(loss=keras_genomics.losses.ambig_binary_crossentropy, optimizer=adam, metrics=['accuracy'])
  48:         print("compiled!")
  49:         sys.stdout.flush()
  50: 
  51: 
  52:         # added to collect optimization results
  53:         if 'results' not in globals():
  54:             global results
  55:             results = []
  56: 
  57:         result = model.fit(x_train,y_train,
  58:                            batch_size=200,
  59:                            epochs=20,
  60:                            verbose=2,
  61:                            validation_data=(x_validate,y_validate))
  62:         print("trained!")
  63:         sys.stdout.flush()
  64: 
  65:         loss,acc = model.evaluate(x_validate,y_validate,verbose=2)
  66:         print("Validation loss:",loss,"Validation acc:",acc)
  67:         sys.stdout.flush()
  68: 
  69:         # added to collect results
  70:         valLoss = result.history['val_loss'][-1]
  71:         parameters = space
  72:         parameters["loss"] = valLoss
  73:         parameters["time"] = int(time.time() - start)
  74:         results.append(parameters)
  75:         print(parameters)
  76:         if len(results) % 10 == 0 :
  77:             tab = tabulate.tabulate(results, headers="keys", tablefmt="fancy_grid", floatfmt=".8f")
  78:             print(tab.encode('utf-8'))
  79:         else:
  80:             tab = tabulate.tabulate(results[-1:], headers="keys", tablefmt="fancy_grid", floatfmt=".8f")
  81:             print(tab.encode('utf-8'))
  82:         print("model %d done ----------------" % len(results))
  83:         sys.stdout.flush()
  84: 
  85:     except:
  86:         loss=1000
  87:         acc=0
  88:         print("failed to run model")
  89:         sys.stdout.flush()
  90: 
  91:         model=None
  92: 
  93:     return{'loss':loss,'status':STATUS_OK,'model':model}
  94: 
compiled!
╒════════════╤════════════════╤════════════╤═════════╤════════════╤══════════╤════════════╤═══════════╤════════╤═════════════╤═════════════╕
│       loss │   kernel_size1 │   n_conv_1 │   Dense │    Dropout │   n_conv │         lr │   filters │   time │   filters_1 │   filters_2 │
╞════════════╪════════════════╪════════════╪═════════╪════════════╪══════════╪════════════╪═══════════╪════════╪═════════════╪═════════════╡
│ 1.06203487 │             39 │          2 │     200 │ 0.60000000 │        0 │ 0.01000000 │       100 │   2203 │         250 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.34995277 │             13 │          1 │      50 │ 0.40000000 │        3 │ 0.01000000 │        50 │  43285 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.97790521 │             19 │          0 │     100 │ 0.60000000 │        0 │ 0.00100000 │       250 │   3218 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.42930689 │             19 │          0 │     100 │ 0.60000000 │        0 │ 0.00010000 │        50 │   1169 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.81242410 │             19 │          0 │      50 │ 0.40000000 │        3 │ 0.00100000 │       500 │  16169 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.21358877 │             15 │          3 │     200 │ 0.60000000 │        1 │ 0.01000000 │       500 │  11702 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.34907818 │             15 │          1 │     200 │ 0.20000000 │        3 │ 0.00100000 │        50 │  47132 │         500 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.24774606 │             15 │          0 │      50 │ 0.20000000 │        3 │ 0.00100000 │        50 │   6325 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.07533299 │             19 │          2 │     200 │ 0.20000000 │        2 │ 0.01000000 │        50 │   3255 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.42409971 │             13 │          1 │     100 │ 0.20000000 │        0 │ 0.01000000 │      1000 │  10923 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.11055320 │             15 │          3 │     200 │ 0.40000000 │        1 │ 0.01000000 │       100 │  20202 │        1000 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.58995342 │             19 │          1 │     100 │ 0.40000000 │        1 │ 0.00100000 │       100 │  11475 │         500 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.41275399 │             39 │          2 │     200 │ 0.20000000 │        2 │ 0.01000000 │       250 │   7987 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.03227169 │             15 │          1 │      50 │ 0.60000000 │        1 │ 0.01000000 │        50 │   3062 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.27641763 │             25 │          3 │     200 │ 0.60000000 │        2 │ 0.00010000 │        50 │  37778 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.81003857 │             19 │          0 │     100 │ 0.20000000 │        2 │ 0.00010000 │      1000 │  40180 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.72277459 │             13 │          0 │     100 │ 0.60000000 │        1 │ 0.00010000 │       500 │  15575 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.43470386 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00100000 │        50 │   1716 │         250 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.47526714 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1713 │         250 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.47209631 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1728 │         250 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41610911 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1785 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.42864420 │             25 │          0 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1431 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41426898 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1797 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41787785 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1759 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.44264716 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │       100 │   2193 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.62410910 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │       250 │   3774 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.25852091 │             39 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │      1000 │  11215 │         100 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.90212774 │             25 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │       500 │   6599 │         100 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.42267183 │             13 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │       100 │   2197 │        1000 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41259948 │             25 │          3 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1909 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41249640 │             25 │          3 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1906 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.67426062 │             39 │          3 │     100 │ 0.40000000 │        0 │ 0.00010000 │       250 │   3884 │          50 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.74366814 │             13 │          3 │      50 │ 0.40000000 │        3 │ 0.00010000 │       500 │  10619 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41284948 │             25 │          3 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   2034 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.96966397 │             25 │          3 │      50 │ 0.40000000 │        3 │ 0.00100000 │        50 │   4664 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.66959511 │             39 │          3 │     200 │ 0.60000000 │        0 │ 0.00010000 │       250 │   3890 │          50 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.14028904 │             19 │          3 │     100 │ 0.40000000 │        1 │ 0.00100000 │       500 │  10061 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.80604251 │             15 │          3 │      50 │ 0.20000000 │        3 │ 0.00010000 │        50 │   4304 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.82757732 │             13 │          3 │     200 │ 0.40000000 │        2 │ 0.00010000 │      1000 │  16830 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.43619362 │             25 │          3 │     100 │ 0.60000000 │        0 │ 0.00100000 │       100 │   2447 │         500 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.99724193 │             19 │          3 │     100 │ 0.20000000 │        3 │ 0.01000000 │        50 │   4469 │          50 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.54965960 │             15 │          3 │     200 │ 0.40000000 │        1 │ 0.00010000 │        50 │   2753 │          50 │          10 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.69193204 │             39 │          1 │      50 │ 0.40000000 │        0 │ 0.00100000 │       250 │   3661 │        1000 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 1.24519927 │             25 │          3 │     100 │ 0.20000000 │        2 │ 0.01000000 │       100 │  40978 │         500 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 1.19869379 │             15 │          1 │     200 │ 0.60000000 │        0 │ 0.00010000 │      1000 │  11585 │          50 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.79398199 │             25 │          3 │     100 │ 0.40000000 │        1 │ 0.01000000 │        50 │   2884 │          50 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 7.71984563 │             13 │          0 │     100 │ 0.60000000 │        2 │ 0.00100000 │        50 │  24414 │         500 │          10 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
47 models 1-47 best is #31

├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.41249640 │             25 │          3 │     100 │ 0.40000000 │        0 │ 0.00010000 │        50 │   1906 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤

