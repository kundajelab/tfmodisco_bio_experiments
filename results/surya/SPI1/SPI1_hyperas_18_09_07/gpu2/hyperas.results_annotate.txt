Evalutation of best performing model:
[0.4548069357648744, 0.5783340269897376]
Best performing model chosen hyper-parameters index:
{'kernel_size1': 0, 'n_conv_1': 0, 'Dense': 1, 'Dropout': 1, 'n_conv': 0, 'lr': 2, 'filters': 2, 'filters_1': 0, 'filters_2': 1}

{'kernel_size1': 0=13, 'n_conv_1': 0=0, 'Dense': 1=100, 'Dropout': 1=0.4, 'n_conv': 0=0, 'lr': 2=0.0001, 'filters': 2=250, 'filters_1': 0=50, 'filters_2': 1=30}

def get_space():
    return {
        'kernel_size1': hp.choice('kernel_size1', [13, 15, 19, 25, 39]),
        'filters': hp.choice('filters', [50,100,250,500,1000]),
        'n_conv': hp.choice('n_conv', [0,1,2,3]),
        'filters_1': hp.choice('filters_1', [50,100,250,500,1000]),
        'filters_2': hp.choice('filters_2', [10,30,60]),
        'n_conv_1': hp.choice('n_conv_1', [0,1,2,3]),
        'Dense': hp.choice('Dense', [50,100,200]),
        'Dropout': hp.choice('Dropout', [0.2,0.4,0.6]),
        'lr': hp.choice('lr', [0.01, 0.001, 0.0001]),
    }

>>> Data
  1: 
  2: np.random.seed(4321)  # for reproducibility
  3: data=np.load('../data_big.npz')
  4: x_train=data['arr_0']
  5: y_train=data['arr_1']
  6: x_validate=data['arr_2']
  7: y_validate=data['arr_3']
  8: x_test=data['arr_4']
  9: y_test=data['arr_5']
 10: 
 11: 
 12: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     start = int(time.time())
   4:     np.random.seed(4321)
   5:     try:
   6:         '''
   7:         b) with and without a hidden fully-connected layer, 
   8:         c) number of units in the hidden fc layer < 200, 
   9:         c) different learning rates for adam (explore on a log scale - 0.001, 0.0001, etc), 
  10:         d) maxpooling widths in the 10-60 range, 
  11:         e) conv widths in the 10-40 range.
  12:         '''
  13:         model=Sequential()
  14:         kernel_size1 = space['kernel_size1']
  15:         kernel_size2 = kernel_size1 - 2
  16:         model.add(Conv1D(filters=space['filters'],kernel_size=(kernel_size1),input_shape=(1000,4)))
  17:         model.add(BatchNormalization(axis=-1))
  18:         model.add(Activation('relu'))
  19: 
  20:         ## a) number of layers between 1 and 4, 
  21: 
  22:         #decide on how many conv layers in model 
  23:         n_conv = space['n_conv']
  24: 
  25:         filter_dim=[kernel_size1,kernel_size2,kernel_size2]
  26: 
  27:         for i in range(n_conv):
  28:             model.add(Conv1D(filters=space['filters_1'],kernel_size=(filter_dim[i])))
  29:             model.add(BatchNormalization(axis=-1))
  30:             model.add(Activation('relu'))
  31: 
  32:         model.add(MaxPooling1D(pool_size=(space['filters_2'])))
  33: 
  34:         model.add(Flatten())
  35:         n_dense = space['n_conv_1']
  36:         for i in range(n_dense):
  37:             model.add(Dense(space['Dense']))
  38:             model.add(BatchNormalization(axis=-1))
  39:             model.add(Activation('relu'))
  40:             model.add(Dropout(space['Dropout']))
  41: 
  42:         model.add(Dense(4))
  43:         model.add(Activation("sigmoid"))
  44: 
  45:         adam=keras.optimizers.Adam(lr=space['lr'])
  46: 
  47:         model.compile(loss=keras_genomics.losses.ambig_binary_crossentropy, optimizer=adam, metrics=['accuracy'])
  48:         print("compiled!")
  49:         sys.stdout.flush()
  50: 
  51: 
  52:         # added to collect optimization results
  53:         if 'results' not in globals():
  54:             global results
  55:             results = []
  56: 
  57:         result = model.fit(x_train,y_train,
  58:                            batch_size=200,
  59:                            epochs=20,
  60:                            verbose=2,
  61:                            validation_data=(x_validate,y_validate))
  62:         print("trained!")
  63:         sys.stdout.flush()
  64: 
  65:         loss,acc = model.evaluate(x_validate,y_validate,verbose=2)
  66:         print("Validation loss:",loss,"Validation acc:",acc)
  67:         sys.stdout.flush()
  68: 
  69:         # added to collect results
  70:         valLoss = result.history['val_loss'][-1]
  71:         parameters = space
  72:         parameters["loss"] = valLoss
  73:         parameters["time"] = int(time.time() - start)
  74:         results.append(parameters)
  75:         print(parameters)
  76:         if len(results) % 10 == 0 :
  77:             tab = tabulate.tabulate(results, headers="keys", tablefmt="fancy_grid", floatfmt=".8f")
  78:             print(tab.encode('utf-8'))
  79:         else:
  80:             tab = tabulate.tabulate(results[-1:], headers="keys", tablefmt="fancy_grid", floatfmt=".8f")
  81:             print(tab.encode('utf-8'))
  82:         print("model %d done ----------------" % len(results))
  83:         sys.stdout.flush()
  84: 
  85:     except:
  86:         loss=1000
  87:         acc=0
  88:         print("failed to run model")
  89:         sys.stdout.flush()
  90: 
  91:         model=None
  92: 
  93:     return{'loss':loss,'status':STATUS_OK,'model':model}
  94: 
c

Summary of best model:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_65 (Conv1D)           (None, 988, 250)          13250     
_________________________________________________________________
batch_normalization_101 (Bat (None, 988, 250)          1000      
_________________________________________________________________
activation_134 (Activation)  (None, 988, 250)          0         
_________________________________________________________________
max_pooling1d_34 (MaxPooling (None, 32, 250)           0         
_________________________________________________________________
flatten_34 (Flatten)         (None, 8000)              0         
_________________________________________________________________
dense_70 (Dense)             (None, 4)                 32004     
_________________________________________________________________
activation_135 (Activation)  (None, 4)                 0         
=================================================================
Total params: 46,254
Trainable params: 45,754
Non-trainable params: 500
_________________________________________________________________
╒════════════╤════════════════╤════════════╤═════════╤════════════╤══════════╤════════════╤═══════════╤════════╤═════════════╤═════════════╕
│       loss │   kernel_size1 │   n_conv_1 │   Dense │    Dropout │   n_conv │         lr │   filters │   time │   filters_1 │   filters_2 │
╞════════════╪════════════════╪════════════╪═════════╪════════════╪══════════╪════════════╪═══════════╪════════╪═════════════╪═════════════╡
│ 1.38254346 │             19 │          3 │      50 │ 0.40000000 │        1 │ 0.00100000 │        50 │   8917 │         500 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.63154782 │             13 │          3 │     200 │ 0.60000000 │        3 │ 0.00010000 │        50 │   4088 │          50 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.69579037 │             39 │          1 │      50 │ 0.60000000 │        0 │ 0.00100000 │       250 │   3425 │        1000 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.28519242 │             15 │          2 │     200 │ 0.20000000 │        2 │ 0.00100000 │       100 │  12114 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.44643104 │             13 │          0 │     200 │ 0.20000000 │        0 │ 0.00010000 │       100 │   1625 │         100 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.71988492 │             15 │          3 │     100 │ 0.60000000 │        1 │ 0.01000000 │        50 │   3310 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.37073249 │             19 │          1 │      50 │ 0.20000000 │        2 │ 0.01000000 │        50 │  32236 │         500 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.78451108 │             15 │          0 │      50 │ 0.20000000 │        3 │ 0.00100000 │        50 │   6292 │         100 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.69319148 │             13 │          0 │     100 │ 0.60000000 │        3 │ 0.00010000 │       250 │   6435 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.44016552 │             39 │          0 │     200 │ 0.20000000 │        0 │ 0.00100000 │       100 │   1879 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 2.81678331 │             13 │          1 │      50 │ 0.20000000 │        1 │ 0.00100000 │       250 │  29207 │        1000 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.68326224 │             25 │          2 │      50 │ 0.60000000 │        1 │ 0.00100000 │        50 │   5371 │         250 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.03403226 │             25 │          3 │      50 │ 0.60000000 │        1 │ 0.00100000 │      1000 │  38780 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.46412368 │             15 │          3 │     200 │ 0.20000000 │        0 │ 0.00010000 │        50 │   1583 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 4.38871165 │             39 │          0 │      50 │ 0.20000000 │        2 │ 0.01000000 │       100 │ 177042 │        1000 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.32329958 │             19 │          3 │      50 │ 0.40000000 │        1 │ 0.00100000 │       500 │  19102 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.28109596 │             13 │          0 │     200 │ 0.20000000 │        3 │ 0.00100000 │        50 │  15721 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.33451596 │             19 │          1 │      50 │ 0.40000000 │        0 │ 0.00010000 │      1000 │  11492 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.30417790 │             19 │          3 │      50 │ 0.20000000 │        2 │ 0.01000000 │        50 │  12257 │         250 │          60 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.45581202 │             39 │          0 │     200 │ 0.20000000 │        0 │ 0.00010000 │       100 │   2004 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.47335053 │             39 │          0 │     200 │ 0.20000000 │        0 │ 0.00010000 │       100 │   1906 │         100 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.42997278 │             13 │          0 │     200 │ 0.40000000 │        0 │ 0.00010000 │       100 │   1893 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.45809412 │             39 │          0 │     200 │ 0.40000000 │        0 │ 0.00010000 │       100 │   1974 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.50607481 │             39 │          0 │     200 │ 0.40000000 │        0 │ 0.00010000 │       500 │   6165 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.45982022 │             13 │          2 │     100 │ 0.40000000 │        0 │ 0.00100000 │       100 │   2074 │         100 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.44054564 │             25 │          0 │     200 │ 0.40000000 │        0 │ 0.00010000 │       100 │   1664 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.45142986 │             39 │          0 │     200 │ 0.40000000 │        0 │ 0.00100000 │       100 │   1716 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.43217674 │             13 │          0 │     100 │ 0.40000000 │        0 │ 0.00010000 │       500 │   5941 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.75984185 │             13 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │       500 │   6325 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.36757383 │             13 │          0 │     100 │ 0.40000000 │        3 │ 0.00010000 │       500 │  60410 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.43269117 │             13 │          0 │     100 │ 0.40000000 │        0 │ 0.00010000 │       500 │   6035 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.69105865 │             13 │          2 │     100 │ 0.40000000 │        0 │ 0.00010000 │       500 │   6387 │         500 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.42764171 │             13 │          0 │     100 │ 0.40000000 │        0 │ 0.00010000 │       250 │   3363 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.67693834 │             13 │          0 │     100 │ 0.40000000 │        3 │ 0.00010000 │       250 │   6439 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.66983889 │             13 │          1 │     100 │ 0.40000000 │        0 │ 0.00010000 │       250 │   3497 │          50 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.13206434 │             15 │          2 │     100 │ 0.40000000 │        2 │ 0.01000000 │       250 │   6227 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.44591414 │             25 │          0 │     200 │ 0.60000000 │        0 │ 0.00010000 │       250 │   3370 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.58421030 │             13 │          0 │     100 │ 0.40000000 │        1 │ 0.00010000 │       250 │   4941 │          50 │          10 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.01742913 │             13 │          3 │     200 │ 0.60000000 │        3 │ 0.01000000 │       250 │   6814 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 1.35997058 │             15 │          1 │     100 │ 0.40000000 │        0 │ 0.00010000 │      1000 │  11469 │        1000 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.56937324 │             13 │          0 │     200 │ 0.40000000 │        2 │ 0.00010000 │       250 │   5664 │          50 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.71474852 │             25 │          0 │     100 │ 0.60000000 │        1 │ 0.00010000 │       100 │   4235 │         100 │          10 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 1.26300982 │             13 │          2 │     200 │ 0.40000000 │        0 │ 0.01000000 │       250 │   3687 │        1000 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.87031730 │             15 │          3 │     100 │ 0.60000000 │        3 │ 0.00010000 │      1000 │  24713 │         100 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.58633976 │             13 │          1 │     200 │ 0.40000000 │        1 │ 0.00010000 │       100 │   3003 │          50 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 4.09947342 │             19 │          0 │     200 │ 0.40000000 │        0 │ 0.01000000 │       250 │   3233 │         100 │          10 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.61850980 │             25 │          0 │     100 │ 0.60000000 │        2 │ 0.00010000 │        50 │   3383 │          50 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.43192661 │             13 │          3 │      50 │ 0.40000000 │        0 │ 0.00100000 │       100 │   2275 │        1000 │          60 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
│ 0.98445556 │             15 │          0 │     200 │ 0.40000000 │        3 │ 0.00010000 │      1000 │  24148 │         100 │          30 │
╘════════════╧════════════════╧════════════╧═════════╧════════════╧══════════╧════════════╧═══════════╧════════╧═════════════╧═════════════╛
1-49 models best # 33
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
│ 0.42764171 │             13 │          0 │     100 │ 0.40000000 │        0 │ 0.00010000 │       250 │   3363 │          50 │          30 │
├────────────┼────────────────┼────────────┼─────────┼────────────┼──────────┼────────────┼───────────┼────────┼─────────────┼─────────────┤
